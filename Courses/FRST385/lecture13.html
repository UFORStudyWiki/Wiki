<!--
title: Lecture 13: frequency analysis
description: 
published: true
date: 2021-12-17T20:29:29.049Z
tags: 
editor: ckeditor
dateCreated: 2021-12-16T18:23:01.592Z
-->

<h1>Objectives</h1>
<ul>
  <li>estimate the probability and risk of damage to populations and infrastructure</li>
  <li>understand impacts of weather and land change impacts on extreme events</li>
</ul>
<p>&nbsp;</p>
<h1>Reason</h1>
<p>flooding is the most destructive disaster worldwide. Most common. Incredibly costly.</p>
<p>&nbsp;</p>
<h1>Definitions</h1>
<p>probability density function (pdf): a function that roughly models a distribution.</p>
<p>return period: the time it takes on average for Q to happen</p>
<p>&nbsp;</p>
<p>Q: discharge&nbsp;<br>P: probability<br>Tr: return period</p>
<p>&nbsp;</p>
<h1>Equations</h1>
<ul>
  <li>p = 1/tr</li>
  <li>Tr = 1/p</li>
</ul>
<h1>Nuances</h1>
<p><br>area under the probability density curve is always 1</p>
<p>normal distribution only has 2 variables (mean and variance).</p>
<p>some distributions, such as Gumbel, have a third parameter (skewness).</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h1>Assumptions of flood frequency analysis</h1>
<ol>
  <li>Time series of floods is a set of independent observations.. This assures that a hydrologic event such as a single large storm does not enter the data set more than once. A single storm system may produce two or more runoff peaks, only the largest should enter the data set.</li>
  <li>The time series of floods is homogeneous. This assures that all the flood observations are from the same population, for instance:<ol>
      <li>Stream gauge has not been relocated</li>
      <li>Land use change has not occured</li>
      <li>No structures have been placed on the stream or its tributaries</li>
      <li>There are no major natural lakes upstream of the stream gauge</li>
      <li>Climate has not changed or changes are accounted for</li>
    </ol>
  </li>
</ol>
<h1>Two methods for frequency analysis</h1>
<ul>
  <li>Graphical method:<ul>
      <li>Using data records to estimate Tr and P within the observed data.<ul>
          <li>Rank magnitude from largest to smallest. Largest = 1; smallest = n</li>
          <li>Calculate exceedance probability [p = m / (n + 1)] &nbsp;or return period [(Tr) = (n + 1) / m]</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Analytical method<ul>
      <li>Using a theoretical pdf to extrapolate beyond the observed record</li>
      <li>Assigning probabilities to events, where every event has a probability of occurrence</li>
      <li>Every location has a specific probability density function (pdf), but we don't know it<ul>
          <li>Fitting and testing pdf: we assume the overall shape of the pdf but use the data to obtain the parameters</li>
          <li>Different distributions may give equally good fit to the sample sample of observed floods but may have different extrapolation capabilities leading to significantly different estimates of extreme foods such as 100-year event.</li>
        </ul>
      </li>
      <li>Selection of appropriate pdf:<ul>
          <li>Larger the dataset, the more parameters that can be included.</li>
          <li>One approach is to test various pdf, and select the one with the best fit.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Some considerations:<ul>
      <li>Frequency analysis is only valid when conditions are stable.</li>
      <li>Changes in mean and variance caused by variability in climate and land use can have a significant impact on extreme events.</li>
      <li>Graphic method works best in the middle range of the dataset; uncertainties grow in the extremes.</li>
      <li>Sample size and pdf selection are critical.</li>
      <li>What is the link between the number of parameters of a pdf and the sample size.</li>
    </ul>
  </li>
</ul>
<h1>Applications, examples, and uses</h1>
<p>Risk (R): Probability of exceeding an event at least once in the next n successive years.</p>
<p>&nbsp;</p>
<p>Probability of non-exceedance (q):</p>
<p>q = 1 - p</p>
<p>Probability of non-exceedance in n successive years:</p>
<p>q^n = (1 - P)^n</p>
<p>Risk (R):</p>
<p>1 - (1 - p)^n = 1 - (1-(1/Tr))^n</p>
<p>&nbsp;</p>
<p>Example: the lifetime of a stream crossing is 25 years; the stream crossing was originally designed based on the 100-yr design flood. What is the risk of failure of the stream crossing in its lifetime?</p>
<p>R = 1-(1-(1/Tr))^n = 1-(1-(1/100))^25 = 0.222 = 22.2%</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2>Risk-based design</h2>
<ul>
  <li>Projects involve risks to environment, property, and life</li>
  <li>When danger to human life is absent, the design will focus on minimizing total expected cost.</li>
</ul>
<p>&nbsp;</p>
<h2>Frequency-based design</h2>
<ul>
  <li>Selection of a frequency or return period for the design of a structure is most often based on potential damage to the environment, property, danger to life, and economic loss</li>
  <li>Forest practices code requires all stream crossings under forest roads to be designed to convey the 100-year design flood</li>
</ul>
<p>&nbsp;</p>
<h1>Chronological Pairing</h1>
<p>Two reasonably similar watersheds are selected for comparison: one to serve as a control, the other to undergo a treatment such as logging. At certain time intervals (e.g. the year 1993) or at certain dates (e.g. April 7th, 1993), some aspect of each watershed - such as peak flow - is measured. The fact that these measurements are made and compared at the same time is the distinguishing characteristic of chronological pairing, and is what allows you to construct the graphs where the peak flow of one watershed is on the X axis while the peak flow of another watershed is on the Y axis: each point on that graph represents a moment in time. This approach seems fairly intuitive and logical on the face of it because it appears to control for certain environmental factors: the assumption is that if one year has an especially snowy winter and the paired watersheds are close together, then you would expect to see that reflected in both the control watershed and the treatment watershed. Younes, however, claims that the underlying assumption that paired watersheds will be in similar states at similar times is incorrect because there’s a high degree of stochasticity in natural phenomena: just because the one watershed has a big snowpack in a given year doesn’t mean that the other will. Because of this stochasticity chronologically pairing is an “uncontrolled experiment”: no one’s checked that every potential confounding variable - such as antecedent soil moisture – is being controlled; therefore, you can’t say whether or not a hydrological response is due to a treatment or some other reason. Frequency pairing fixes this problem by doing away with the time element. Rather than asking “how big were the floods in watershed A versus the floods in watershed B in years C, D, and E after it was logged?”, the question becomes “since we logged watershed A, how have its biggest floods compared to watershed B, and how often have those occurred?” The idea is that, for stochastic reasons, watershed A might be more prone to having big floods in 1993 while watershed B might be more prone to having big floods in 1994, so you compare the floods that these years produce, rather than comparing watershed A in 1993 with watershed B in 1993.</p>
<p>Another way to think about it: If you look at the graphs associated with CP, the peak flows of two watersheds are paired as a function of time (i.e. time is on the X-axis); so the question is "how big was the peak flow of each watershed at time T?". When you look at the graphs for frequency pairing, return interval is on the X-axis, so the question is "how big is teh 100-year peak flow of watershed A versus watershed B?". CP assumes that time controls for all pertinent variables; with FP, you know that all stochastic variables are being controlled for by comparing between years.</p>
