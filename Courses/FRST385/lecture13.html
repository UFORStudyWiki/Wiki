<!--
title: Lecture 13: frequency analysis
description: 
published: true
date: 2021-12-16T19:49:32.585Z
tags: 
editor: ckeditor
dateCreated: 2021-12-16T18:23:01.592Z
-->

<h1>Objectives</h1>
<ul>
  <li>estimate the probability and risk of damage to populations and infrastructure</li>
  <li>understand impacts of weather and land change impacts on extreme events</li>
</ul>
<p>&nbsp;</p>
<h1>Reason</h1>
<p>flooding is the most destructive disaster worldwide. Most common. Incredibly costly.</p>
<p>&nbsp;</p>
<h1>Definitions</h1>
<p>probability density function (pdf): a function that roughly models a distribution.</p>
<p>return period: the time it takes on average for Q to happen</p>
<p>&nbsp;</p>
<p>Q: discharge&nbsp;<br>P: probability<br>Tr: return period</p>
<p>&nbsp;</p>
<h1>Equations</h1>
<ul>
  <li>p = 1/tr</li>
  <li>Tr = 1/p</li>
</ul>
<h1>Nuances</h1>
<p><br>area under the probability density curve is always 1</p>
<p>normal distribution only has 2 variables (mean and variance).</p>
<p>some distributions, such as Gumbel, have a third parameter (skewness).</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h1>Assumptions of flood frequency analysis</h1>
<ol>
  <li>Time series of floods is a set of independent observations.. This assures that a hydrologic event such as a single large storm does not enter the data set more than once. A single storm system may produce two or more runoff peaks, only the largest should enter the data set.</li>
  <li>The time series of floods is homogeneous. This assures that all the flood observations are from the same population, for instance:<ol>
      <li>Stream gauge has not been relocated</li>
      <li>Land use change has not occured</li>
      <li>No structures have been placed on the stream or its tributaries</li>
      <li>There are no major natural lakes upstream of the stream gauge</li>
      <li>Climate has not changed or changes are accounted for</li>
    </ol>
  </li>
</ol>
<h1>Two methods for frequency analysis</h1>
<ul>
  <li>Graphical method:<ul>
      <li>Using data records to estimate Tr and P within the observed data.<ul>
          <li>Rank magnitude from largest to smallest. Largest = 1; smallest = n</li>
          <li>Calculate exceedance probability [p = m / (n + 1)] &nbsp;or return period [(Tr) = (n + 1) / m]&nbsp;</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Analytical method<ul>
      <li>Using a theoretical pdf to extrapolate beyond the observed record</li>
      <li>Assigning probabilities to events, where every event has a probability of occurrence</li>
      <li>Every location has a specific probability density function (pdf), but we don't know it<ul>
          <li>Fitting and testing pdf: we assume the overall shape of the pdf but use the data to obtain the parameters</li>
          <li>Different distributions may give equally good fit to the sample sample of observed floods but may have different extrapolation capabilities leading to significantly different estimates of extreme foods such as 100-year event.</li>
        </ul>
      </li>
      <li>Selection of appropriate pdf:<ul>
          <li>Larger the dataset, the more parameters that can be included.</li>
          <li>One approach is to test various pdf, and select the one with the best fit.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Some considerations:<ul>
      <li>Frequency analysis is only valid when conditions are stable.</li>
      <li>Changes in mean and variance caused by variability in climate and land use can have a significant impact on extreme events.</li>
      <li>Graphic method works best in the middle range of the dataset; uncertainties grow in the extremes.</li>
      <li>Sample size and pdf selection are critical.</li>
      <li>What is the link between the number of parameters of a pdf and the sample size.</li>
      <li>&nbsp;</li>
    </ul>
  </li>
</ul>
<p>&nbsp;</p>
